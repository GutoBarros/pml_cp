---
title: "Practical Machine Learning Course Project"
author: "Jose Augusto Barros de Oliveira"
date: "02/01/2021"
output:
  rmdformats::robobook:
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library (skimr)
library(caret)
library(lubridate)
library(corrplot)
library(tidyverse)
```

## Summary  

The goal of this project is predict whether weight-lift exercises were performed as recommended. It uses the experimental design and data from [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/har). (Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human â€™13) . Stuttgart, Germany: ACM SIGCHI, 2013).  

The dataset contains 160 features and 19622 cases. The data was splited in two sets, the training one with 70% of cases. After some Exploratory Data Analysis (EDA), we decide to go forward modeling with 60 features, besides the target. We performed Principal Component Analysis (PCA) in order to reduce the dimensionality, (20 components explained 90% of the variance), but we gave up that approach, looking at the trade-off between dimensionality, interpretability and acuracy.

We chose a XGBoost model and used cross validation (CV) with K = 10 to tuning parameters.
The overall model accuracy on test set (out sample) was 99.7%, a significant improvement over the original study performance (98.2%) . The main features in the model were *row_belt*, *yaw_belt*, *magnet_dumbell_z* and *pitch_forearm* (50% of total variable importance).


## Data wrangling and first analysis

The data was ingested and splitted in two sets, the training one with 70% and the testing with 30%. 

```{r dataingestion}
# Data Ingestion
theURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

if(file_test("-f", "pml-training.csv") == FALSE) 
  {
  download.file(theURL,"pml-training.csv",mode = "wb")
}

traindata <- read.csv("pml-training.csv", na.strings = c("","NA","NULL"))


# Spliting
set.seed(1969)
indice <- createDataPartition(y= traindata$classe, p = .70, list=FALSE)
training <- traindata[indice,]
testing <- traindata[-indice,]

```
It seems that there are two classes of features, regarding their origin. The majority relates to the measures of the exercises and the others are metadata from the experiments (e.g. user names e timestamp).  

### Metadata features  

We found a perfect correlation between *raw_timestamp_part_1* and *cvtd_timestamp*, meaning that they express the same thing, so we can drop one of them

```{r plot1}

g1 <- ggplot(data=training, aes(x=as.numeric(raw_timestamp_part_1), y = dmy_hm(cvtd_timestamp))) +
  geom_point(shape =1, position=position_jitter(width=2,height=2)) +
  labs(x = "raw_timestamp_part_1", y = "cvtd_timestamp") +
  ggtitle ("raw_timestamp_part_1 and cvtd_timestamp")
g1

```

We found an almost deterministic relationship between *num_window* and *classe*, suggesting that the experimental design carried out series of repeated measures by *classe* and *user_name*, as showed next plot.

```{r plot2}
temp <- training %>%
  count(classe, num_window,user_name)

g2 <- ggplot(temp, aes(fill=classe, y=n, x=num_window)) + 
    geom_bar(position="fill", stat="identity") + facet_grid (user_name ~ .) +
  theme(axis.text.x= element_text(size = 8, angle = 90)) + ggtitle ("Proportion of cases by num_window, user_name and classe")
rm(temp)
g2
```

Thus, *num_window* by itself is enough to predict *classe* with zero error on training data and a really impressive accuracy on testing data:

```{r random_forest_to_num_window}
if(file_test("-f", "ml_rf.RDS") == FALSE) 
  {
  ml_rf <- train(classe ~ num_window + user_name, method = "rf", data = training)
  saveRDS(ml_rf, "ml_rf.RDS")
}
ml_rf <- readRDS("ml_rf.RDS")

# in sample performance
confusionMatrix(predict(ml_rf, newdata = training[,-160]), reference = as.factor(training[,160]))

#outsample performance
confusionMatrix(predict(ml_rf, newdata = testing[,-160]), reference = as.factor(testing[,160]))
```



### Measurement features

Although the instructions for this project grants the use of any feature available in the training dataset, we decide to stick with the principles of the original paper and kept only the measurement variables, as the original main goal was to use the measures of movement as predictors, not the metadata. So, we decided to drop the metadata features and go on with the measurement ones.

```{r assessing_completness_of_measurement _features}
# Data assessment
dataquality <- skim_without_charts(training[,-c(1:7)])
head(dataquality,10)
```

There are `r nrow(subset(dataquality, complete_rate < 0.022)) `  features with complete rate near to 0.02. This missingness is probably Missing at Random (MAR), since there is a pattern, being the amount of missing values the same across the features and cases. We choose to drop those features because it's hard to make a good inference on their values based on so scarce data. 

```{r feature selection}

temp <- subset(dataquality, complete_rate > 0.022)
training <- training[, temp$skim_variable]
rm(temp)

# final dataset to modeling
colnames(training)

```

## Pre processing

Some variables are high correlated:

```{r correlations}
corrplot(cor(training[,-1]),
         method = "square",
         type = "upper", diag = FALSE,
         tl.cex = 0.6)

```

We have tried Principal Component Analysis (PCA) to reduce dimensionality. We kept 90% of the original variance with 20 components, but neither accuracy and interpretability were better than the model without PCA. So, preprocessing was just centering and scaling the variables.

```{r preprocessing}
preProc <- preProcess(training[, -1], method = c("center", "scale"))
preProc



training <- predict(preProc, training)
```


## Modeling

A XGBoost model was trained on the prepared data set using the Caret package. We used the method "xgbtree" that needs seven parameters:  

* nrounds (# Boosting Iterations)
* max_depth (Max Tree Depth)
* eta (Shrinkage)
* gamma (Minimum Loss Reduction)
* colsample_bytree (Subsample Ratio of Columns)
* min_child_weight (Minimum Sum of Instance Weight)
* subsample (Subsample Percentage)

The grid search for those parameters comprised 243 models (see [The Caret Reference, Basic parameter Tuning](https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning)), each of then was cross validated in ten folds (k = 10).

```{r xgboost_model}
fitControl <- trainControl (method = "repeatedcv",
                            number = 10,
                            repeats = 10)

if(file_test("-f", "ml_xgboost2.RDS") == FALSE) 
  {
  ml_xgboost2 <- train(classe ~ .,
                    method = "xgbTree",
                    trControl = fitControl,
                    data = training)
  saveRDS(ml_xgboost2, "ml_xgboost2.RDS")
}
ml_xgboost2 <- readRDS("ml_xgboost2.RDS")

```



```{r}
ml_xgboost2$finalModel
```

The final parameters were:  

* nrounds = 150
* max_depth = 3
* eta = 0.4
* gamma = 0
* colsample_bytree = 0.8
* min_child_weight = 1
* subsample = 0.75

### Model's performance

The in sample performance was
```{r in_sample_perfomance}
confusionMatrix(predict(ml_xgboost2, newdata = training[,-1]), reference = as.factor(training[,1]))

```

The out of sample performance was

```{r out_sample_perfomance}
temp <- subset(dataquality, complete_rate > 0.022)
testing <- testing[, temp$skim_variable]
rm(temp)

testing <- predict(preProc, testing) # preprocessing testdata using preprocess from training data

cm <- confusionMatrix(predict(ml_xgboost2, newdata = testing[,-1]), reference = as.factor(testing[,1]))
cm
cm <- as.data.frame((cm$byClass))
Avg_precision <- mean(cm$`Pos Pred Value`)
```


The importance of each variable can be assessed in the next table

```{r}
varImp(ml_xgboost2)
```


The acuracy and averaged precision reached 99,7%. This values can be compared with the original paper ones, 98,2% in both measures. This improvement in performance may be credited to the algorithm itself and the carefully tunning of parameters, what was achieved with a reasonably computational cost (circa 5 hours to trainining the model).

## Prediction and final assessment

```{r}
# Load test data

theURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(file_test("-f", "pml-testing.csv") == FALSE) 
{
  download.file(theURL,"pml-testing.csv",mode = "wb")
}

testdata <- read.csv("pml-testing.csv", na.strings = c("","NA","NULL"))

names.use <- names(testdata)[(names(testdata) %in% dataquality$skim_variable)]

testdata <- testdata[ , names.use]

temp <- subset(dataquality, complete_rate > 0.022)
temp <- temp[-1,] # drop the target
testing <- testdata[, temp$skim_variable]
rm(temp)

# Preprocessing

testing <- predict(preProc, testing)

predicted <- predict(ml_xgboost2, newdata = testing)
predicted
```






